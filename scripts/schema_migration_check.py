#!/usr/bin/env python3
"""
Schemaè¿ç§»ä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬
è‡ªåŠ¨éƒ¨ç½²åæ£€æŸ¥æ‰€æœ‰æ•°æ®åº“schemaè¿ç§»æ˜¯å¦ä¸€è‡´ï¼Œç¡®ä¿ä¸æ¼ä»»ä½•ç»“æ„å˜æ›´
"""

import os
import sys
import json
import asyncio
import hashlib
from typing import Dict, List, Set, Optional, Tuple
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from database.db_connection import db_manager
from database.db_init import DatabaseInitializer


class SchemaMigrationChecker:
    """Schemaè¿ç§»ä¸€è‡´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.info: List[str] = []
        self.expected_schema_version = None
        self.actual_schema_version = None
        self.migration_files: List[Path] = []
        
    def load_expected_schema_version(self) -> str:
        """ä»ä»£ç ä¸­è·å–æœŸæœ›çš„schemaç‰ˆæœ¬"""
        try:
            from database.db_init import DatabaseInitializer
            initializer = DatabaseInitializer()
            self.expected_schema_version = initializer.current_schema_version
            self.info.append(f"âœ… æœŸæœ›Schemaç‰ˆæœ¬: {self.expected_schema_version}")
            return self.expected_schema_version
        except Exception as e:
            self.errors.append(f"âŒ æ— æ³•è·å–æœŸæœ›Schemaç‰ˆæœ¬: {e}")
            return None
    
    async def get_actual_schema_version(self) -> Optional[str]:
        """ä»æ•°æ®åº“è·å–å®é™…çš„schemaç‰ˆæœ¬"""
        try:
            query = "SELECT config_value FROM system_config WHERE config_key = 'schema_version'"
            result = await db_manager.fetch_one(query)
            
            if result:
                self.actual_schema_version = result['config_value']
                self.info.append(f"âœ… å®é™…Schemaç‰ˆæœ¬: {self.actual_schema_version}")
                return self.actual_schema_version
            else:
                self.errors.append("âŒ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°schema_versioné…ç½®")
                return None
                
        except Exception as e:
            self.errors.append(f"âŒ è·å–æ•°æ®åº“Schemaç‰ˆæœ¬å¤±è´¥: {e}")
            return None
    
    def scan_migration_files(self) -> List[Path]:
        """æ‰«ææ‰€æœ‰è¿ç§»æ–‡ä»¶"""
        migration_dir = Path(__file__).parent.parent / "database" / "migrations"
        
        if not migration_dir.exists():
            self.warnings.append("âš ï¸  migrationsç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç›®å½•")
            migration_dir.mkdir(parents=True, exist_ok=True)
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰è¿ç§»æ–‡ä»¶
        migration_files = list(migration_dir.glob("migration_*.sql"))
        migration_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
        
        self.migration_files = migration_files
        self.info.append(f"âœ… æ‰¾åˆ° {len(migration_files)} ä¸ªè¿ç§»æ–‡ä»¶")
        
        return migration_files
    
    def validate_migration_files(self) -> bool:
        """éªŒè¯è¿ç§»æ–‡ä»¶æ ¼å¼å’Œå®Œæ•´æ€§"""
        print("ğŸ” éªŒè¯è¿ç§»æ–‡ä»¶...")
        
        valid = True
        
        for migration_file in self.migration_files:
            try:
                # æ£€æŸ¥æ–‡ä»¶åæ ¼å¼
                if not self._validate_migration_filename(migration_file.name):
                    self.errors.append(f"âŒ è¿ç§»æ–‡ä»¶åæ ¼å¼é”™è¯¯: {migration_file.name}")
                    valid = False
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                content = migration_file.read_text(encoding='utf-8')
                if not content.strip():
                    self.errors.append(f"âŒ è¿ç§»æ–‡ä»¶ä¸ºç©º: {migration_file.name}")
                    valid = False
                    continue
                
                # æ£€æŸ¥SQLè¯­æ³•åŸºæœ¬ç»“æ„
                if not self._validate_sql_content(content):
                    self.warnings.append(f"âš ï¸  è¿ç§»æ–‡ä»¶SQLå¯èƒ½æœ‰é—®é¢˜: {migration_file.name}")
                
                self.info.append(f"âœ… è¿ç§»æ–‡ä»¶æœ‰æ•ˆ: {migration_file.name}")
                
            except Exception as e:
                self.errors.append(f"âŒ è¯»å–è¿ç§»æ–‡ä»¶å¤±è´¥ {migration_file.name}: {e}")
                valid = False
        
        return valid
    
    def _validate_migration_filename(self, filename: str) -> bool:
        """éªŒè¯è¿ç§»æ–‡ä»¶åæ ¼å¼"""
        # æ ¼å¼: migration_YYYY_MM_DD_N_description.sql
        import re
        pattern = r'^migration_\d{4}_\d{2}_\d{2}_\d+_.*\.sql$'
        return re.match(pattern, filename) is not None
    
    def _validate_sql_content(self, content: str) -> bool:
        """åŸºæœ¬SQLå†…å®¹éªŒè¯"""
        content_upper = content.upper()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬SQLå…³é”®å­—
        sql_keywords = ['CREATE', 'ALTER', 'INSERT', 'UPDATE', 'DROP']
        has_sql = any(keyword in content_upper for keyword in sql_keywords)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„SQLè¯­æ³•é”™è¯¯æ ‡å¿—
        has_basic_structure = (
            content.count('(') == content.count(')') and
            content.count("'") % 2 == 0 and
            content.count('"') % 2 == 0
        )
        
        return has_sql and has_basic_structure
    
    async def check_table_structure_consistency(self) -> bool:
        """æ£€æŸ¥è¡¨ç»“æ„ä¸€è‡´æ€§"""
        print("ğŸ” æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„...")
        
        try:
            # å®šä¹‰æœŸæœ›çš„æ ¸å¿ƒè¡¨å’Œé‡è¦å­—æ®µ
            expected_tables = {
                'merchants': {
                    'required_fields': [
                        'id', 'chat_id', 'name', 'region', 'category', 
                        'status', 'created_at', 'updated_at',
                        'merchant_type', 'province_id', 'city_id', 
                        'p_price', 'pp_price', 'custom_description',
                        'user_info', 'channel_link'
                    ],
                    'constraints': ['chat_id UNIQUE']
                },
                'orders': {
                    'required_fields': [
                        'id', 'user_id', 'username', 'merchant_id', 
                        'order_type', 'price', 'status', 'created_at'
                    ],
                    'foreign_keys': ['merchant_id -> merchants(id)']
                },
                'binding_codes': {
                    'required_fields': [
                        'id', 'code', 'is_used', 'merchant_id', 
                        'created_at', 'expires_at'
                    ],
                    'constraints': ['code UNIQUE']
                },
                'auto_reply_triggers': {
                    'required_fields': [
                        'id', 'trigger_text', 'match_type', 'is_active',
                        'created_by', 'admin_id', 'created_at', 'updated_at'
                    ]
                },
                'auto_reply_messages': {
                    'required_fields': [
                        'id', 'trigger_id', 'message_content', 'is_active',
                        'created_at', 'updated_at'
                    ],
                    'foreign_keys': ['trigger_id -> auto_reply_triggers(id)']
                },
                'provinces': {
                    'required_fields': [
                        'id', 'name', 'display_order', 'is_active',
                        'created_at', 'updated_at', 'code'
                    ],
                    'constraints': ['name UNIQUE']
                },
                'cities': {
                    'required_fields': [
                        'id', 'name', 'province_id', 'display_order',
                        'is_active', 'created_at', 'updated_at'
                    ],
                    'foreign_keys': ['province_id -> provinces(id)'],
                    'constraints': ['UNIQUE(name, province_id)']
                },
                'keywords': {
                    'required_fields': [
                        'id', 'name', 'display_order', 'is_active',
                        'created_at', 'updated_at'
                    ],
                    'constraints': ['name UNIQUE']
                },
                'merchant_keywords': {
                    'required_fields': ['id', 'merchant_id', 'keyword_id', 'created_at'],
                    'foreign_keys': [
                        'merchant_id -> merchants(id)',
                        'keyword_id -> keywords(id)'
                    ],
                    'constraints': ['UNIQUE(merchant_id, keyword_id)']
                },
                'templates': {
                    'required_fields': ['key', 'content', 'updated_at'],
                    'constraints': ['key PRIMARY KEY']
                },
                'system_config': {
                    'required_fields': [
                        'id', 'config_key', 'config_value', 
                        'description', 'updated_at'
                    ],
                    'constraints': ['config_key UNIQUE']
                }
            }
            
            # æ£€æŸ¥æ¯ä¸ªè¡¨
            all_tables_valid = True
            
            for table_name, expectations in expected_tables.items():
                table_valid = await self._check_single_table(table_name, expectations)
                all_tables_valid = all_tables_valid and table_valid
            
            return all_tables_valid
            
        except Exception as e:
            self.errors.append(f"âŒ è¡¨ç»“æ„æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def _check_single_table(self, table_name: str, expectations: Dict) -> bool:
        """æ£€æŸ¥å•ä¸ªè¡¨çš„ç»“æ„"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            table_exists_query = """
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name=?
            """
            table_result = await db_manager.fetch_one(table_exists_query, (table_name,))
            
            if not table_result:
                self.errors.append(f"âŒ è¡¨ä¸å­˜åœ¨: {table_name}")
                return False
            
            # è·å–è¡¨ç»“æ„ä¿¡æ¯
            table_info_query = f"PRAGMA table_info({table_name})"
            columns = await db_manager.fetch_all(table_info_query)
            
            if not columns:
                self.errors.append(f"âŒ æ— æ³•è·å–è¡¨ç»“æ„: {table_name}")
                return False
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            existing_columns = {col['name'] for col in columns}
            required_fields = expectations.get('required_fields', [])
            
            missing_fields = []
            for field in required_fields:
                if field not in existing_columns:
                    missing_fields.append(field)
            
            if missing_fields:
                self.errors.append(f"âŒ {table_name} ç¼ºå°‘å­—æ®µ: {missing_fields}")
                return False
            
            self.info.append(f"âœ… è¡¨ç»“æ„æ­£ç¡®: {table_name} ({len(existing_columns)} å­—æ®µ)")
            return True
            
        except Exception as e:
            self.errors.append(f"âŒ æ£€æŸ¥è¡¨ {table_name} å¤±è´¥: {e}")
            return False
    
    async def check_indexes_and_constraints(self) -> bool:
        """æ£€æŸ¥ç´¢å¼•å’Œçº¦æŸ"""
        print("ğŸ” æ£€æŸ¥æ•°æ®åº“ç´¢å¼•å’Œçº¦æŸ...")
        
        try:
            # æœŸæœ›çš„ç´¢å¼•
            expected_indexes = [
                'idx_merchants_chat_id',
                'idx_merchants_status',
                'idx_orders_user_id',
                'idx_orders_merchant_id',
                'idx_binding_codes_code',
                'idx_provinces_name',
                'idx_cities_province_id',
                'idx_keywords_name',
                'idx_merchant_keywords_merchant_id',
                'idx_auto_reply_triggers_active'
            ]
            
            # è·å–å®é™…ç´¢å¼•
            indexes_query = """
                SELECT name FROM sqlite_master 
                WHERE type='index' AND name NOT LIKE 'sqlite_%'
            """
            actual_indexes = await db_manager.fetch_all(indexes_query)
            actual_index_names = {idx['name'] for idx in actual_indexes}
            
            # æ£€æŸ¥ç¼ºå¤±çš„ç´¢å¼•
            missing_indexes = []
            for expected_idx in expected_indexes:
                if expected_idx not in actual_index_names:
                    missing_indexes.append(expected_idx)
            
            if missing_indexes:
                self.warnings.append(f"âš ï¸  ç¼ºå°‘ç´¢å¼•: {missing_indexes}")
            
            self.info.append(f"âœ… æ‰¾åˆ° {len(actual_index_names)} ä¸ªç´¢å¼•")
            
            # æ£€æŸ¥è§¦å‘å™¨
            triggers_query = """
                SELECT name FROM sqlite_master 
                WHERE type='trigger'
            """
            triggers = await db_manager.fetch_all(triggers_query)
            self.info.append(f"âœ… æ‰¾åˆ° {len(triggers)} ä¸ªè§¦å‘å™¨")
            
            return len(missing_indexes) == 0
            
        except Exception as e:
            self.errors.append(f"âŒ æ£€æŸ¥ç´¢å¼•å’Œçº¦æŸå¤±è´¥: {e}")
            return False
    
    async def check_data_integrity(self) -> bool:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        print("ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        try:
            integrity_checks = []
            
            # æ£€æŸ¥å¤–é”®å¼•ç”¨å®Œæ•´æ€§
            fk_checks = [
                ("orders.merchant_id", "SELECT COUNT(*) FROM orders WHERE merchant_id NOT IN (SELECT id FROM merchants)"),
                ("cities.province_id", "SELECT COUNT(*) FROM cities WHERE province_id NOT IN (SELECT id FROM provinces)"),
                ("merchant_keywords.merchant_id", "SELECT COUNT(*) FROM merchant_keywords WHERE merchant_id NOT IN (SELECT id FROM merchants)"),
                ("merchant_keywords.keyword_id", "SELECT COUNT(*) FROM merchant_keywords WHERE keyword_id NOT IN (SELECT id FROM keywords)"),
            ]
            
            for check_name, query in fk_checks:
                try:
                    result = await db_manager.fetch_one(query)
                    invalid_count = result[list(result.keys())[0]] if result else 0
                    
                    if invalid_count > 0:
                        self.errors.append(f"âŒ æ•°æ®å®Œæ•´æ€§é”™è¯¯: {check_name} æœ‰ {invalid_count} æ¡æ— æ•ˆå¼•ç”¨")
                        integrity_checks.append(False)
                    else:
                        self.info.append(f"âœ… å¤–é”®å®Œæ•´æ€§æ­£å¸¸: {check_name}")
                        integrity_checks.append(True)
                        
                except Exception as e:
                    self.warnings.append(f"âš ï¸  æ— æ³•æ£€æŸ¥ {check_name}: {e}")
            
            # æ£€æŸ¥å…³é”®é…ç½®æ˜¯å¦å­˜åœ¨
            config_checks = [
                'schema_version',
                'bot_status',
                'auto_reply_enabled'
            ]
            
            for config_key in config_checks:
                config_query = "SELECT config_value FROM system_config WHERE config_key = ?"
                result = await db_manager.fetch_one(config_query, (config_key,))
                
                if result:
                    self.info.append(f"âœ… é…ç½®å­˜åœ¨: {config_key} = {result['config_value']}")
                else:
                    self.warnings.append(f"âš ï¸  é…ç½®ç¼ºå¤±: {config_key}")
            
            return all(integrity_checks)
            
        except Exception as e:
            self.errors.append(f"âŒ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def calculate_schema_hash(self) -> str:
        """è®¡ç®—schemaç»“æ„å“ˆå¸Œå€¼"""
        try:
            # æ”¶é›†æ‰€æœ‰schemaæ–‡ä»¶å†…å®¹
            schema_files = [
                Path(__file__).parent.parent / "database" / "schema.sql",
                Path(__file__).parent.parent / "database" / "schema_extended.sql",
                Path(__file__).parent.parent / "database" / "schema_auto_reply.sql"
            ]
            
            combined_content = ""
            for schema_file in schema_files:
                if schema_file.exists():
                    combined_content += schema_file.read_text(encoding='utf-8')
            
            # æ·»åŠ è¿ç§»æ–‡ä»¶å†…å®¹
            for migration_file in self.migration_files:
                combined_content += migration_file.read_text(encoding='utf-8')
            
            # è®¡ç®—å“ˆå¸Œ
            schema_hash = hashlib.sha256(combined_content.encode('utf-8')).hexdigest()[:16]
            self.info.append(f"âœ… Schemaå“ˆå¸Œå€¼: {schema_hash}")
            
            return schema_hash
            
        except Exception as e:
            self.warnings.append(f"âš ï¸  æ— æ³•è®¡ç®—Schemaå“ˆå¸Œ: {e}")
            return "unknown"
    
    def generate_migration_report(self) -> Dict:
        """ç”Ÿæˆè¿ç§»æ£€æŸ¥æŠ¥å‘Š"""
        return {
            'timestamp': datetime.now().isoformat(),
            'expected_schema_version': self.expected_schema_version,
            'actual_schema_version': self.actual_schema_version,
            'version_match': self.expected_schema_version == self.actual_schema_version,
            'migration_files_count': len(self.migration_files),
            'schema_hash': self.calculate_schema_hash(),
            'total_checks': len(self.errors) + len(self.warnings) + len(self.info),
            'errors': len(self.errors),
            'warnings': len(self.warnings),
            'status': 'SUCCESS' if len(self.errors) == 0 else 'FAILED',
            'details': {
                'errors': self.errors,
                'warnings': self.warnings,
                'info': self.info,
                'migration_files': [f.name for f in self.migration_files]
            }
        }
    
    def print_results(self) -> bool:
        """æ‰“å°æ£€æŸ¥ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“‹ Schemaè¿ç§»ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ")
        print("="*80)
        
        # ç‰ˆæœ¬å¯¹æ¯”
        print(f"\nğŸ“Š ç‰ˆæœ¬å¯¹æ¯”:")
        print(f"  æœŸæœ›ç‰ˆæœ¬: {self.expected_schema_version}")
        print(f"  å®é™…ç‰ˆæœ¬: {self.actual_schema_version}")
        
        if self.expected_schema_version == self.actual_schema_version:
            print("  âœ… ç‰ˆæœ¬ä¸€è‡´")
        else:
            print("  âŒ ç‰ˆæœ¬ä¸åŒ¹é…ï¼")
        
        # æ‰“å°ä¿¡æ¯
        if self.info:
            print("\nâœ… æ£€æŸ¥é€šè¿‡:")
            for msg in self.info:
                print(f"  {msg}")
        
        # æ‰“å°è­¦å‘Š
        if self.warnings:
            print("\nâš ï¸  è­¦å‘Šä¿¡æ¯:")
            for msg in self.warnings:
                print(f"  {msg}")
        
        # æ‰“å°é”™è¯¯
        if self.errors:
            print("\nâŒ é”™è¯¯ä¿¡æ¯:")
            for msg in self.errors:
                print(f"  {msg}")
        
        # æ€»ç»“
        print("\n" + "="*80)
        if self.errors:
            print("âŒ Schemaè¿ç§»æ£€æŸ¥å¤±è´¥ï¼æ•°æ®åº“ç»“æ„ä¸æœŸæœ›ä¸ä¸€è‡´ã€‚")
            print("\nğŸ”§ å»ºè®®æ“ä½œ:")
            print("  1. æ£€æŸ¥æ•°æ®åº“è¿ç§»æ˜¯å¦æ­£ç¡®æ‰§è¡Œ")
            print("  2. éªŒè¯æ‰€æœ‰è¿ç§»æ–‡ä»¶æ˜¯å¦å·²åº”ç”¨")
            print("  3. æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæƒé™")
            print("  4. é‡æ–°è¿è¡Œæ•°æ®åº“åˆå§‹åŒ–")
            return False
        elif self.warnings:
            print("âš ï¸  Schemaè¿ç§»æ£€æŸ¥åŸºæœ¬é€šè¿‡ï¼Œä½†æœ‰è­¦å‘Šéœ€è¦æ³¨æ„ã€‚")
            print("\nğŸ’¡ å»ºè®®:")
            print("  1. æŸ¥çœ‹è­¦å‘Šä¿¡æ¯å¹¶è¯„ä¼°å½±å“")
            print("  2. è€ƒè™‘æ·»åŠ ç¼ºå¤±çš„ç´¢å¼•")
            print("  3. ç›‘æ§æ•°æ®åº“æ€§èƒ½")
            return True
        else:
            print("âœ… Schemaè¿ç§»æ£€æŸ¥å®Œå…¨é€šè¿‡ï¼æ•°æ®åº“ç»“æ„å®Œå…¨ä¸€è‡´ã€‚")
            print("\nğŸ‰ æ•°æ®åº“çŠ¶æ€:")
            print("  âœ“ æ‰€æœ‰è¡¨ç»“æ„æ­£ç¡®")
            print("  âœ“ ç´¢å¼•å’Œçº¦æŸå®Œæ•´")
            print("  âœ“ æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
            print("  âœ“ ç‰ˆæœ¬å·åŒ¹é…")
            return True


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Schemaè¿ç§»ä¸€è‡´æ€§æ£€æŸ¥")
    print("="*80)
    print("æ£€æŸ¥æ•°æ®åº“schemaæ˜¯å¦ä¸ä»£ç æœŸæœ›ä¸€è‡´ï¼Œç¡®ä¿è¿ç§»å®Œæ•´æ€§")
    
    checker = SchemaMigrationChecker()
    
    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    try:
        await db_manager.initialize()
        print("âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return 1
    
    try:
        # æ‰§è¡Œæ£€æŸ¥æ­¥éª¤
        print(f"\nğŸ” å¼€å§‹Schemaæ£€æŸ¥...")
        
        # 1. è·å–ç‰ˆæœ¬ä¿¡æ¯
        expected_version = checker.load_expected_schema_version()
        actual_version = await checker.get_actual_schema_version()
        
        # 2. æ‰«æè¿ç§»æ–‡ä»¶
        migration_files = checker.scan_migration_files()
        
        # 3. éªŒè¯è¿ç§»æ–‡ä»¶
        migration_files_valid = checker.validate_migration_files()
        
        # 4. æ£€æŸ¥è¡¨ç»“æ„
        table_structure_valid = await checker.check_table_structure_consistency()
        
        # 5. æ£€æŸ¥ç´¢å¼•å’Œçº¦æŸ
        indexes_valid = await checker.check_indexes_and_constraints()
        
        # 6. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        data_integrity_valid = await checker.check_data_integrity()
        
        # ç»Ÿè®¡ç»“æœ
        all_checks = [
            ("ç‰ˆæœ¬åŒ¹é…", expected_version == actual_version),
            ("è¿ç§»æ–‡ä»¶", migration_files_valid),
            ("è¡¨ç»“æ„", table_structure_valid),
            ("ç´¢å¼•çº¦æŸ", indexes_valid),
            ("æ•°æ®å®Œæ•´æ€§", data_integrity_valid)
        ]
        
        passed = sum(1 for name, result in all_checks if result)
        total = len(all_checks)
        
        print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆ: {passed}/{total} é¡¹é€šè¿‡")
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        success = checker.print_results()
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        report = checker.generate_migration_report()
        report_file = Path(__file__).parent.parent / 'schema_migration_check.json'
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ æ£€æŸ¥æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print(f"\nâš ï¸  æ— æ³•ä¿å­˜æ£€æŸ¥æŠ¥å‘Š: {e}")
        
        return 0 if success else 1
        
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        try:
            await db_manager.close()
            print("\nğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except:
            pass


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)