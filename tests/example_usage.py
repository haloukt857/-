#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæµ‹è¯•ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Telegramå•†æˆ·æœºå™¨äººV2.0ç»¼åˆæµ‹è¯•ç³»ç»Ÿçš„å„ç§åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from tests.config.test_config import TestConfig, ConfigTemplates
from tests.utils.test_helpers import TestResultCollector, TestReporter, PerformanceMonitor
from tests.run_comprehensive_tests import ComprehensiveTestRunner


async def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ§ª åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("-" * 50)
    
    # 1. åˆ›å»ºé»˜è®¤é…ç½®çš„æµ‹è¯•è¿è¡Œå™¨
    runner = ComprehensiveTestRunner()
    
    # 2. è¿è¡Œç¯å¢ƒæ£€æŸ¥
    print("ğŸ” è¿è¡Œç¯å¢ƒæ£€æŸ¥...")
    env_ok = await runner.run_environment_check()
    if not env_ok:
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥")
        return
    
    # 3. åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
    print("ğŸš€ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")
    init_ok = await runner.initialize_test_environment()
    if not init_ok:
        print("âŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")
        return
    
    # 4. è¿è¡Œå•ä¸ªæ¨¡å—æµ‹è¯•
    print("ğŸ§ª è¿è¡Œç®¡ç†å‘˜åå°æµ‹è¯•æ¨¡å—...")
    result = await runner.run_single_module("admin_backend")
    
    print(f"âœ… æµ‹è¯•å®Œæˆ - çŠ¶æ€: {result.get('status')}")
    if "total_tests" in result:
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡: {result['passed_tests']}/{result['total_tests']} é€šè¿‡")
    
    # 5. æ¸…ç†
    await runner.cleanup()


async def example_custom_config():
    """è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    print("\nâš™ï¸ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹")
    print("-" * 50)
    
    # 1. åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    config = TestConfig()
    config.debug_mode = True
    config.verbose_output = True
    config.max_workers = 2
    config.enable_performance_monitoring = False
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    config_path = PROJECT_ROOT / "tests" / "config" / "example_config.json"
    config.save(str(config_path))
    print(f"ğŸ“ é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # 2. ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºè¿è¡Œå™¨
    runner = ComprehensiveTestRunner(str(config_path))
    
    print(f"ğŸ”§ ä½¿ç”¨é…ç½®: {runner.config}")
    
    # 3. è¿è¡ŒæŒ‡å®šæ¨¡å—
    modules_to_run = ["admin_backend", "user_experience"]
    print(f"ğŸ¯ è®¡åˆ’è¿è¡Œæ¨¡å—: {modules_to_run}")
    
    # è¿™é‡Œåªæ˜¯æ¼”ç¤ºï¼Œä¸å®é™…è¿è¡Œ
    print("âœ… é…ç½®ç¤ºä¾‹å®Œæˆ")


async def example_performance_monitoring():
    """æ€§èƒ½ç›‘æ§ç¤ºä¾‹"""
    print("\nğŸ“ˆ æ€§èƒ½ç›‘æ§ç¤ºä¾‹")
    print("-" * 50)
    
    # 1. åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor()
    
    # 2. å¯åŠ¨ç›‘æ§
    monitor.start()
    print("ğŸ”„ æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    # 3. æ¨¡æ‹Ÿä¸€äº›å·¥ä½œ
    await asyncio.sleep(2)
    
    # 4. æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡
    monitor.add_custom_metric("test_duration", 1.5)
    monitor.add_custom_metric("test_memory_usage", 45.2)
    
    # 5. è·å–æŒ‡æ ‡
    metrics = monitor.get_metrics()
    print("ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    for metric_name, metric_value in metrics.items():
        print(f"  {metric_name}: {metric_value}")
    
    # 6. åœæ­¢ç›‘æ§
    monitor.stop()
    print("â¹ï¸ æ€§èƒ½ç›‘æ§å·²åœæ­¢")


async def example_result_collection():
    """ç»“æœæ”¶é›†ç¤ºä¾‹"""
    print("\nğŸ“Š ç»“æœæ”¶é›†ç¤ºä¾‹")
    print("-" * 50)
    
    # 1. åˆ›å»ºç»“æœæ”¶é›†å™¨
    collector = TestResultCollector()
    
    # 2. å¼€å§‹æ”¶é›†
    collector.start_collection()
    
    # 3. æ·»åŠ æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
    collector.add_test_result("test_database_connection", "PASSED", 0.5)
    collector.add_test_result("test_user_creation", "PASSED", 1.2)
    collector.add_test_result("test_invalid_input", "FAILED", 0.3, "Validation error")
    collector.add_test_result("test_performance", "PASSED", 2.1)
    
    # 4. æ·»åŠ æ¨¡å—ç»“æœ
    module_result = {
        "status": "PASSED",
        "total_tests": 4,
        "passed_tests": 3,
        "failed_tests": 1,
        "duration": 4.1
    }
    collector.add_module_result("example_module", module_result)
    
    # 5. æ·»åŠ æ€§èƒ½æ•°æ®
    collector.add_performance_data("cpu_usage", 35.5, "%")
    collector.add_performance_data("memory_usage", 128.3, "MB")
    
    # 6. ç»“æŸæ”¶é›†
    collector.end_collection()
    
    # 7. è·å–æ‘˜è¦
    summary = collector.get_summary()
    print(f"ğŸ“‹ æµ‹è¯•æ‘˜è¦:")
    print(f"  æ€»æµ‹è¯•æ•°: {summary.total_tests}")
    print(f"  é€šè¿‡æµ‹è¯•: {summary.passed_tests}")
    print(f"  å¤±è´¥æµ‹è¯•: {summary.failed_tests}")
    print(f"  é€šè¿‡ç‡: {summary.pass_rate:.1f}%")
    print(f"  æ€»è€—æ—¶: {summary.total_duration:.2f}ç§’")
    
    # 8. å¯¼å‡ºä¸ºJSON
    json_file = PROJECT_ROOT / "tests" / "reports" / "example_results.json"
    collector.export_to_json(str(json_file))
    print(f"ğŸ’¾ ç»“æœå·²å¯¼å‡ºåˆ°: {json_file}")


async def example_report_generation():
    """æŠ¥å‘Šç”Ÿæˆç¤ºä¾‹"""
    print("\nğŸ“„ æŠ¥å‘Šç”Ÿæˆç¤ºä¾‹")
    print("-" * 50)
    
    # 1. åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    reporter = TestReporter(output_dir="tests/reports")
    
    # 2. å‡†å¤‡ç¤ºä¾‹æŠ¥å‘Šæ•°æ®
    report_data = {
        "summary": {
            "execution_time": 125.6,
            "total_modules": 3,
            "passed_modules": 2,
            "failed_modules": 1,
            "module_pass_rate": 66.7,
            "total_tests": 25,
            "passed_tests": 22,
            "failed_tests": 3,
            "test_pass_rate": 88.0
        },
        "results": {
            "admin_backend": {
                "status": "PASSED",
                "total_tests": 10,
                "passed_tests": 10,
                "duration": 45.2,
                "pass_rate": 100.0
            },
            "user_experience": {
                "status": "PASSED",
                "total_tests": 8,
                "passed_tests": 7,
                "duration": 52.1,
                "pass_rate": 87.5
            },
            "merchant_onboarding": {
                "status": "FAILED",
                "total_tests": 7,
                "passed_tests": 5,
                "duration": 28.3,
                "pass_rate": 71.4,
                "error": "FSMçŠ¶æ€æœºå®ç°ç¼ºé™·"
            }
        },
        "environment": {
            "python_version": "3.12.0",
            "platform": "darwin",
            "timestamp": "2025-09-13T15:30:00"
        },
        "performance_metrics": {
            "cpu_avg": 42.3,
            "cpu_max": 68.1,
            "memory_avg": 156.7,
            "memory_max": 203.4
        }
    }
    
    # 3. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report_files = await reporter.generate_comprehensive_report(report_data)
    
    print("ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ:")
    for report_type, file_path in report_files.items():
        print(f"  {report_type}: {file_path}")


async def example_config_templates():
    """é…ç½®æ¨¡æ¿ç¤ºä¾‹"""
    print("\nğŸ“ é…ç½®æ¨¡æ¿ç¤ºä¾‹")
    print("-" * 50)
    
    # 1. å¼€å‘ç¯å¢ƒé…ç½®
    dev_config = ConfigTemplates.development()
    print(f"ğŸ”§ å¼€å‘ç¯å¢ƒé…ç½®: {dev_config}")
    
    # 2. CI/CDé…ç½®
    ci_config = ConfigTemplates.ci_cd()
    print(f"ğŸš€ CI/CDé…ç½®: {ci_config}")
    
    # 3. æ€§èƒ½æµ‹è¯•é…ç½®
    perf_config = ConfigTemplates.performance()
    print(f"ğŸ“Š æ€§èƒ½æµ‹è¯•é…ç½®: {perf_config}")
    
    # 4. ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿé…ç½®
    prod_config = ConfigTemplates.production_simulation()
    print(f"ğŸ­ ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿé…ç½®: {prod_config}")
    
    # 5. ä¿å­˜æ‰€æœ‰æ¨¡æ¿é…ç½®
    config_dir = PROJECT_ROOT / "tests" / "config"
    
    templates = {
        "dev_config.json": dev_config,
        "ci_config.json": ci_config,
        "performance_config.json": perf_config,
        "production_sim_config.json": prod_config
    }
    
    for filename, config in templates.items():
        file_path = config_dir / filename
        config.save(str(file_path))
        print(f"ğŸ’¾ å·²ä¿å­˜: {file_path}")


async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¯ Telegramå•†æˆ·æœºå™¨äººV2.0ç»¼åˆæµ‹è¯•ç³»ç»Ÿ - ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)
    
    try:
        # è¿è¡Œå„ä¸ªç¤ºä¾‹
        await example_basic_usage()
        await example_custom_config()
        await example_performance_monitoring()
        await example_result_collection()
        await example_report_generation()
        await example_config_templates()
        
        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹:")
        print("  - tests/README.md - å®Œæ•´ä½¿ç”¨æ–‡æ¡£")
        print("  - tests/config/test_config.py - é…ç½®ç®¡ç†API")
        print("  - tests/utils/test_helpers.py - æµ‹è¯•å·¥å…·API")
        print("  - tests/run_comprehensive_tests.py - ä¸»æµ‹è¯•è¿è¡Œå™¨")
        
    except Exception as e:
        print(f"\nğŸ’¥ ç¤ºä¾‹è¿è¡Œå‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())