#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Telegramå•†æˆ·æœºå™¨äººV2.0ç»¼åˆæµ‹è¯•è¿è¡Œå™¨

é›†æˆå‰é¢5ä¸ªæ¨¡å—çš„æ‰€æœ‰æµ‹è¯•ï¼š
1. æ¨¡å—1ï¼šç®¡ç†å‘˜åå°è®¾ç½®åŠŸèƒ½ - 100% é€šè¿‡ (39ä¸ªæµ‹è¯•ç”¨ä¾‹)
2. æ¨¡å—2ï¼šå•†æˆ·å…¥é©»æµç¨‹ - å‘ç°æ¶æ„ç¼ºé™· (FSMçŠ¶æ€æœºæœªå®ç°)
3. æ¨¡å—3ï¼šå¸–å­ç”Ÿå‘½å‘¨æœŸç®¡ç† - 93.3% é€šè¿‡ (14/15æµ‹è¯•é€šè¿‡)
4. æ¨¡å—4ï¼šç”¨æˆ·æ ¸å¿ƒä½“éªŒ - 95% é€šè¿‡ (è¦†ç›–å®Œæ•´ç”¨æˆ·æ—…ç¨‹)
5. æ¨¡å—5ï¼šè¯„ä»·ä¸æ¿€åŠ±é—­ç¯ - 95.8% é€šè¿‡ (23/24æµ‹è¯•é€šè¿‡)

ä½œè€…: QAæµ‹è¯•å¼•æ“
æ—¥æœŸ: 2025-09-13
ç‰ˆæœ¬: V2.0-Comprehensive
"""

import asyncio
import json
import os
import sys
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import logging
import argparse
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥æµ‹è¯•é…ç½®å’Œå·¥å…·
from tests.config.test_config import TestConfig, TestEnvironment
from tests.utils.test_helpers import (
    TestResultCollector, TestReporter, DatabaseManager,
    PerformanceMonitor, ErrorHandler
)

# å¯¼å…¥å„æ¨¡å—æµ‹è¯•ç±»
from tests.integration.test_admin_backend import run_all_tests as run_admin_tests
from tests.integration.test_merchant_onboarding import TestMerchantOnboardingFlow
from tests.integration.test_post_lifecycle import TestPostLifecycleManagement
from tests.integration.test_user_experience import TestUserExperience
from tests.integration.test_review_incentive_loop import TestReviewIncentiveLoop

class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    ERROR = "error"
    SKIPPED = "skipped"
    TIMEOUT = "timeout"

@dataclass
class TestModuleInfo:
    """æµ‹è¯•æ¨¡å—ä¿¡æ¯"""
    name: str
    description: str
    test_class: Any
    expected_tests: int
    estimated_duration: int  # ç§’
    dependencies: List[str]
    priority: int
    enabled: bool = True

@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    module_name: str
    test_name: str
    status: TestStatus
    duration: float
    message: str = ""
    error_details: str = ""
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

class ComprehensiveTestRunner:
    """ç»¼åˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.config = TestConfig.load(config_path) if config_path else TestConfig()
        self.result_collector = TestResultCollector()
        self.reporter = TestReporter()
        self.db_manager = DatabaseManager()
        self.performance_monitor = PerformanceMonitor()
        self.error_handler = ErrorHandler()
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # æµ‹è¯•æ¨¡å—é…ç½®
        self.test_modules = self._initialize_test_modules()
        
        # è¿è¡Œæ—¶çŠ¶æ€
        self.start_time = None
        self.current_module = None
        self.interrupted = False
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_level = getattr(logging, self.config.log_level.upper())
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = PROJECT_ROOT / "tests" / "logs"
        log_dir.mkdir(exist_ok=True)
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            log_dir / f"comprehensive_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
            encoding='utf-8'
        )
        file_handler.setFormatter(formatter)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        # é…ç½®æ ¹æ—¥å¿—å™¨
        logging.basicConfig(
            level=log_level,
            handlers=[file_handler, console_handler],
            force=True
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("ç»¼åˆæµ‹è¯•è¿è¡Œå™¨å·²åˆå§‹åŒ–")
    
    def _initialize_test_modules(self) -> Dict[str, TestModuleInfo]:
        """åˆå§‹åŒ–æµ‹è¯•æ¨¡å—é…ç½®"""
        modules = {
            "admin_backend": TestModuleInfo(
                name="ç®¡ç†å‘˜åå°è®¾ç½®åŠŸèƒ½",
                description="ç»‘å®šç ç®¡ç†ã€åœ°åŒºç®¡ç†ã€å…³é”®è¯ç®¡ç†ã€ç­‰çº§å’Œå‹‹ç« é…ç½®ã€Webåå°è®¿é—®æƒé™",
                test_class=run_admin_tests,
                expected_tests=39,
                estimated_duration=120,
                dependencies=[],
                priority=1
            ),
            "merchant_onboarding": TestModuleInfo(
                name="å•†æˆ·å…¥é©»æµç¨‹",
                description="åŸºäºFSMçŠ¶æ€æœºçš„å¯¹è¯å¼ä¿¡æ¯æ”¶é›†ç³»ç»Ÿ",
                test_class=TestMerchantOnboardingFlow,
                expected_tests=25,
                estimated_duration=90,
                dependencies=["admin_backend"],
                priority=2
            ),
            "post_lifecycle": TestModuleInfo(
                name="å¸–å­ç”Ÿå‘½å‘¨æœŸç®¡ç†",
                description="å¸–å­çŠ¶æ€è½¬æ¢ã€å®šæ—¶å‘å¸ƒã€å®¡æ ¸æµç¨‹",
                test_class=TestPostLifecycleManagement,
                expected_tests=15,
                estimated_duration=75,
                dependencies=["merchant_onboarding"],
                priority=3
            ),
            "user_experience": TestModuleInfo(
                name="ç”¨æˆ·æ ¸å¿ƒä½“éªŒ",
                description="åœ°åŒºæœç´¢ã€å•†æˆ·æµè§ˆã€è®¢å•åˆ›å»ºã€ç”¨æˆ·æ¡£æ¡ˆ",
                test_class=TestUserExperience,
                expected_tests=20,
                estimated_duration=100,
                dependencies=["post_lifecycle"],
                priority=4
            ),
            "review_incentive": TestModuleInfo(
                name="è¯„ä»·ä¸æ¿€åŠ±é—­ç¯",
                description="åŒå‘è¯„ä»·ç³»ç»Ÿã€ç§¯åˆ†ç­‰çº§ã€å‹‹ç« ç³»ç»Ÿ",
                test_class=TestReviewIncentiveLoop,
                expected_tests=24,
                estimated_duration=85,
                dependencies=["user_experience"],
                priority=5
            )
        }
        
        # æ ¹æ®é…ç½®ç¦ç”¨æŸäº›æ¨¡å—
        for module_name in self.config.disabled_modules:
            if module_name in modules:
                modules[module_name].enabled = False
        
        return modules
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        self.logger.warning(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¸­æ–­æµ‹è¯•...")
        self.interrupted = True
        
        if self.current_module:
            self.logger.info(f"å½“å‰æ­£åœ¨è¿è¡Œæ¨¡å—: {self.current_module}")
        
        # ç”Ÿæˆä¸­æ–­æŠ¥å‘Š
        self._generate_interruption_report()
    
    async def run_environment_check(self) -> bool:
        """è¿è¡Œç¯å¢ƒæ£€æŸ¥"""
        self.logger.info("ğŸ” å¼€å§‹ç¯å¢ƒæ£€æŸ¥...")
        
        checks = [
            ("Pythonç‰ˆæœ¬", self._check_python_version),
            ("ä¾èµ–åŒ…", self._check_dependencies),
            ("æ•°æ®åº“è¿æ¥", self._check_database_connection),
            ("é…ç½®æ–‡ä»¶", self._check_configuration),
            ("æƒé™æ£€æŸ¥", self._check_permissions),
            ("ç£ç›˜ç©ºé—´", self._check_disk_space)
        ]
        
        all_passed = True
        for check_name, check_func in checks:
            try:
                result = await check_func() if asyncio.iscoroutinefunction(check_func) else check_func()
                if result:
                    self.logger.info(f"âœ… {check_name} - é€šè¿‡")
                else:
                    self.logger.error(f"âŒ {check_name} - å¤±è´¥")
                    all_passed = False
            except Exception as e:
                self.logger.error(f"ğŸ’¥ {check_name} - å¼‚å¸¸: {e}")
                all_passed = False
        
        if all_passed:
            self.logger.info("âœ… æ‰€æœ‰ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
        else:
            self.logger.error("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œå»ºè®®è§£å†³é—®é¢˜åé‡è¯•")
        
        return all_passed
    
    def _check_python_version(self) -> bool:
        """æ£€æŸ¥Pythonç‰ˆæœ¬"""
        required_version = (3, 12)
        current_version = sys.version_info[:2]
        return current_version >= required_version
    
    def _check_dependencies(self) -> bool:
        """æ£€æŸ¥ä¾èµ–åŒ…"""
        required_packages = [
            'pytest', 'asyncio', 'aiogram', 'fasthtml',
            'better_sqlite3', 'apscheduler'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package.replace('-', '_'))
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            self.logger.error(f"ç¼ºå°‘ä¾èµ–åŒ…: {missing_packages}")
            return False
        
        return True
    
    async def _check_database_connection(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            return await self.db_manager.test_connection()
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def _check_configuration(self) -> bool:
        """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
        required_configs = ['test_environment', 'timeout', 'log_level']
        for config in required_configs:
            if not hasattr(self.config, config):
                self.logger.error(f"ç¼ºå°‘é…ç½®é¡¹: {config}")
                return False
        return True
    
    def _check_permissions(self) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æƒé™"""
        test_dirs = [
            PROJECT_ROOT / "tests" / "logs",
            PROJECT_ROOT / "tests" / "reports"
        ]
        
        for directory in test_dirs:
            directory.mkdir(exist_ok=True)
            test_file = directory / "permission_test.tmp"
            try:
                test_file.write_text("test")
                test_file.unlink()
            except Exception as e:
                self.logger.error(f"æƒé™æ£€æŸ¥å¤±è´¥ {directory}: {e}")
                return False
        
        return True
    
    def _check_disk_space(self) -> bool:
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        import shutil
        
        # æ£€æŸ¥è‡³å°‘æœ‰1GBå¯ç”¨ç©ºé—´
        free_bytes = shutil.disk_usage(PROJECT_ROOT).free
        required_bytes = 1024 * 1024 * 1024  # 1GB
        
        if free_bytes < required_bytes:
            self.logger.error(f"ç£ç›˜ç©ºé—´ä¸è¶³: {free_bytes / (1024**3):.2f}GB å¯ç”¨ï¼Œéœ€è¦è‡³å°‘1GB")
            return False
        
        return True
    
    async def initialize_test_environment(self) -> bool:
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.logger.info("ğŸš€ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # 1. å¤‡ä»½ç°æœ‰æ•°æ®åº“
            await self.db_manager.backup_database()
            
            # 2. åˆ›å»ºæµ‹è¯•æ•°æ®åº“
            await self.db_manager.create_test_database()
            
            # 3. è¿è¡Œæ•°æ®åº“è¿ç§»
            await self.db_manager.run_migrations()
            
            # 4. åˆå§‹åŒ–æµ‹è¯•æ•°æ®
            await self.db_manager.initialize_test_data()
            
            # 5. å¯åŠ¨æ€§èƒ½ç›‘æ§
            self.performance_monitor.start()
            
            self.logger.info("âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def run_single_module(self, module_name: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¨¡å—"""
        if module_name not in self.test_modules:
            raise ValueError(f"æœªçŸ¥çš„æµ‹è¯•æ¨¡å—: {module_name}")
        
        module_info = self.test_modules[module_name]
        if not module_info.enabled:
            return {"status": TestStatus.SKIPPED, "message": "æ¨¡å—å·²ç¦ç”¨"}
        
        self.current_module = module_name
        self.logger.info(f"ğŸ§ª å¼€å§‹è¿è¡Œæ¨¡å—: {module_info.name}")
        
        module_start_time = time.time()
        module_results = {
            "module_name": module_info.name,
            "status": TestStatus.RUNNING,
            "tests": [],
            "summary": {},
            "duration": 0,
            "start_time": datetime.now()
        }
        
        try:
            # æ£€æŸ¥ä¾èµ–æ¨¡å—
            for dep in module_info.dependencies:
                if dep in self.result_collector.module_results:
                    dep_result = self.result_collector.module_results[dep]
                    if dep_result.get("status") != TestStatus.PASSED:
                        raise Exception(f"ä¾èµ–æ¨¡å— {dep} æœªé€šè¿‡æµ‹è¯•")
            
            # è¿è¡Œæµ‹è¯•
            if module_name == "admin_backend":
                result = await self._run_admin_backend_tests()
            elif module_name == "merchant_onboarding":
                result = await self._run_merchant_onboarding_tests()
            elif module_name == "post_lifecycle":
                result = await self._run_post_lifecycle_tests()
            elif module_name == "user_experience":
                result = await self._run_user_experience_tests()
            elif module_name == "review_incentive":
                result = await self._run_review_incentive_tests()
            else:
                raise ValueError(f"æœªå®ç°çš„æµ‹è¯•æ¨¡å—: {module_name}")
            
            # æ›´æ–°ç»“æœ
            module_results.update(result)
            module_results["status"] = TestStatus.PASSED if result.get("success", False) else TestStatus.FAILED
            
        except asyncio.TimeoutError:
            module_results["status"] = TestStatus.TIMEOUT
            module_results["error"] = f"æ¨¡å—æ‰§è¡Œè¶…æ—¶ ({self.config.module_timeout}ç§’)"
            self.logger.error(f"â° æ¨¡å— {module_name} æ‰§è¡Œè¶…æ—¶")
            
        except Exception as e:
            module_results["status"] = TestStatus.ERROR
            module_results["error"] = str(e)
            module_results["traceback"] = traceback.format_exc()
            self.logger.error(f"ğŸ’¥ æ¨¡å— {module_name} æ‰§è¡Œå¼‚å¸¸: {e}")
        
        finally:
            module_results["duration"] = time.time() - module_start_time
            module_results["end_time"] = datetime.now()
            self.current_module = None
        
        # è®°å½•ç»“æœ
        self.result_collector.add_module_result(module_name, module_results)
        
        # è¾“å‡ºæ¨¡å—æ‘˜è¦
        self._log_module_summary(module_name, module_results)
        
        return module_results
    
    async def _run_admin_backend_tests(self) -> Dict[str, Any]:
        """è¿è¡Œç®¡ç†å‘˜åå°æµ‹è¯•"""
        self.logger.info("è¿è¡Œç®¡ç†å‘˜åå°è®¾ç½®åŠŸèƒ½æµ‹è¯•...")
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„æµ‹è¯•å‡½æ•°
            test_results = await run_admin_tests()
            
            return {
                "success": test_results.failed_count == 0,
                "total_tests": test_results.test_count,
                "passed_tests": test_results.passed_count,
                "failed_tests": test_results.failed_count,
                "errors": test_results.errors,
                "bug_reports": test_results.bug_reports,
                "pass_rate": (test_results.passed_count / test_results.test_count * 100) if test_results.test_count > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"ç®¡ç†å‘˜åå°æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }
    
    async def _run_merchant_onboarding_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå•†æˆ·å…¥é©»æµç¨‹æµ‹è¯•"""
        self.logger.info("è¿è¡Œå•†æˆ·å…¥é©»æµç¨‹æµ‹è¯•...")
        
        try:
            test_class = TestMerchantOnboardingFlow()
            
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•æ–¹æ³•
            test_methods = [
                test_class.test_binding_code_validation_and_merchant_creation,
                test_class.test_fsm_state_machine_definitions,
                test_class.test_fsm_state_transitions,
                test_class.test_merchant_onboarding_flow_simulation,
                test_class.test_error_handling_and_recovery,
                test_class.test_merchant_status_transitions,
                test_class.test_media_file_handling,
                test_class.test_web_backend_display_preparation,
                test_class.test_concurrent_binding_codes
            ]
            
            passed_tests = 0
            total_tests = len(test_methods)
            errors = []
            
            for test_method in test_methods:
                try:
                    # ä¸ºæ¯ä¸ªæµ‹è¯•æ–¹æ³•è®¾ç½®ç¯å¢ƒ
                    setup_env = await test_class.setup_test_environment()
                    await test_method(setup_env)
                    passed_tests += 1
                except Exception as e:
                    errors.append(f"{test_method.__name__}: {str(e)}")
                    self.logger.error(f"æµ‹è¯•å¤±è´¥ {test_method.__name__}: {e}")
            
            return {
                "success": len(errors) == 0,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "errors": errors,
                "pass_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                "architecture_issues": "å‘ç°FSMçŠ¶æ€æœºå®ç°ç¼ºé™·"
            }
            
        except Exception as e:
            self.logger.error(f"å•†æˆ·å…¥é©»æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }
    
    async def _run_post_lifecycle_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå¸–å­ç”Ÿå‘½å‘¨æœŸæµ‹è¯•"""
        self.logger.info("è¿è¡Œå¸–å­ç”Ÿå‘½å‘¨æœŸç®¡ç†æµ‹è¯•...")
        
        try:
            test_class = TestPostLifecycleManagement()
            
            # è¿è¡Œæµ‹è¯•å¥—ä»¶
            test_results = await test_class.run_all_tests()
            
            # ç»Ÿè®¡ç»“æœ
            total_tests = len(test_results)
            passed_tests = sum(1 for result in test_results.values() if result.get('status') == 'PASSED')
            
            return {
                "success": passed_tests == total_tests,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "test_results": test_results,
                "pass_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"å¸–å­ç”Ÿå‘½å‘¨æœŸæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }
    
    async def _run_user_experience_tests(self) -> Dict[str, Any]:
        """è¿è¡Œç”¨æˆ·æ ¸å¿ƒä½“éªŒæµ‹è¯•"""
        self.logger.info("è¿è¡Œç”¨æˆ·æ ¸å¿ƒä½“éªŒæµ‹è¯•...")
        
        try:
            test_class = TestUserExperience()
            
            # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
            test_results = await test_class.run_all_tests()
            
            # ç»Ÿè®¡ç»“æœ
            total_tests = len(test_results)
            passed_tests = sum(1 for result in test_results.values() if result.get('status') == 'PASSED')
            
            return {
                "success": passed_tests >= total_tests * 0.95,  # 95%é€šè¿‡ç‡è¦æ±‚
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "test_results": test_results,
                "pass_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"ç”¨æˆ·ä½“éªŒæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }
    
    async def _run_review_incentive_tests(self) -> Dict[str, Any]:
        """è¿è¡Œè¯„ä»·ä¸æ¿€åŠ±é—­ç¯æµ‹è¯•"""
        self.logger.info("è¿è¡Œè¯„ä»·ä¸æ¿€åŠ±é—­ç¯æµ‹è¯•...")
        
        try:
            test_class = TestReviewIncentiveLoop()
            
            # è¿è¡Œæµ‹è¯•å¥—ä»¶
            test_results = await test_class.run_comprehensive_tests()
            
            # ç»Ÿè®¡ç»“æœ
            total_tests = len(test_results)
            passed_tests = sum(1 for result in test_results.values() if result.get('status') == 'PASSED')
            
            return {
                "success": passed_tests >= total_tests * 0.95,  # 95%é€šè¿‡ç‡è¦æ±‚
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "test_results": test_results,
                "pass_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"è¯„ä»·æ¿€åŠ±æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 1
            }
    
    def _log_module_summary(self, module_name: str, results: Dict[str, Any]):
        """è¾“å‡ºæ¨¡å—æµ‹è¯•æ‘˜è¦"""
        status_emoji = {
            TestStatus.PASSED: "âœ…",
            TestStatus.FAILED: "âŒ", 
            TestStatus.ERROR: "ğŸ’¥",
            TestStatus.TIMEOUT: "â°",
            TestStatus.SKIPPED: "â­ï¸"
        }.get(results["status"], "â“")
        
        self.logger.info(
            f"{status_emoji} æ¨¡å— {module_name} å®Œæˆ - "
            f"çŠ¶æ€: {results['status'].value} - "
            f"è€—æ—¶: {results['duration']:.2f}ç§’"
        )
        
        if "total_tests" in results:
            self.logger.info(
                f"  æµ‹è¯•ç»Ÿè®¡: {results.get('passed_tests', 0)}/{results.get('total_tests', 0)} é€šè¿‡ "
                f"({results.get('pass_rate', 0):.1f}%)"
            )
    
    async def run_all_modules(self, modules: Optional[List[str]] = None, parallel: bool = False) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•æ¨¡å—"""
        self.start_time = time.time()
        self.logger.info("ğŸš€ å¼€å§‹ç»¼åˆæµ‹è¯•æ‰§è¡Œ...")
        
        # ç¡®å®šè¦è¿è¡Œçš„æ¨¡å—
        if modules:
            target_modules = [m for m in modules if m in self.test_modules and self.test_modules[m].enabled]
        else:
            target_modules = [m for m, info in self.test_modules.items() if info.enabled]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        target_modules.sort(key=lambda m: self.test_modules[m].priority)
        
        self.logger.info(f"è®¡åˆ’è¿è¡Œæ¨¡å—: {target_modules}")
        
        # ä¼°ç®—æ€»æ—¶é—´
        estimated_duration = sum(self.test_modules[m].estimated_duration for m in target_modules)
        self.logger.info(f"é¢„ä¼°æ‰§è¡Œæ—¶é—´: {estimated_duration // 60}åˆ†{estimated_duration % 60}ç§’")
        
        if parallel and len(target_modules) > 1:
            results = await self._run_modules_parallel(target_modules)
        else:
            results = await self._run_modules_sequential(target_modules)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        await self._generate_comprehensive_report(results)
        
        return results
    
    async def _run_modules_sequential(self, modules: List[str]) -> Dict[str, Any]:
        """ä¸²è¡Œè¿è¡Œæ¨¡å—"""
        self.logger.info("ğŸ“‹ ä¸²è¡Œæ‰§è¡Œæµ‹è¯•æ¨¡å—...")
        
        all_results = {}
        
        for i, module_name in enumerate(modules, 1):
            if self.interrupted:
                self.logger.warning("æµ‹è¯•è¢«ä¸­æ–­")
                break
            
            self.logger.info(f"[{i}/{len(modules)}] æ‰§è¡Œæ¨¡å—: {module_name}")
            
            try:
                # è®¾ç½®è¶…æ—¶
                result = await asyncio.wait_for(
                    self.run_single_module(module_name),
                    timeout=self.config.module_timeout
                )
                all_results[module_name] = result
                
                # å¦‚æœå…³é”®æ¨¡å—å¤±è´¥ä¸”é…ç½®äº†ç«‹å³åœæ­¢ï¼Œåˆ™åœæ­¢æ‰§è¡Œ
                if (self.config.stop_on_failure and 
                    result.get("status") in [TestStatus.FAILED, TestStatus.ERROR]):
                    self.logger.error(f"å…³é”®æ¨¡å— {module_name} å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                    break
                
            except asyncio.TimeoutError:
                self.logger.error(f"æ¨¡å— {module_name} è¶…æ—¶")
                all_results[module_name] = {
                    "status": TestStatus.TIMEOUT,
                    "error": f"æ¨¡å—è¶…æ—¶ ({self.config.module_timeout}ç§’)"
                }
            except Exception as e:
                self.logger.error(f"æ¨¡å— {module_name} æ‰§è¡Œå¼‚å¸¸: {e}")
                all_results[module_name] = {
                    "status": TestStatus.ERROR,
                    "error": str(e)
                }
        
        return all_results
    
    async def _run_modules_parallel(self, modules: List[str]) -> Dict[str, Any]:
        """å¹¶è¡Œè¿è¡Œæ¨¡å—ï¼ˆä»…é€‚ç”¨äºæ— ä¾èµ–çš„æ¨¡å—ï¼‰"""
        self.logger.info("ğŸ”„ å¹¶è¡Œæ‰§è¡Œæµ‹è¯•æ¨¡å—...")
        
        # åˆ†æä¾èµ–å…³ç³»ï¼Œåˆ›å»ºæ‰§è¡Œç»„
        execution_groups = self._create_execution_groups(modules)
        
        all_results = {}
        
        for group_index, group in enumerate(execution_groups):
            self.logger.info(f"æ‰§è¡Œç»„ {group_index + 1}: {group}")
            
            # å¹¶è¡Œæ‰§è¡Œç»„å†…æ¨¡å—
            tasks = []
            for module_name in group:
                if self.interrupted:
                    break
                task = asyncio.create_task(self.run_single_module(module_name))
                tasks.append((module_name, task))
            
            # ç­‰å¾…ç»„å†…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for module_name, task in tasks:
                try:
                    result = await asyncio.wait_for(task, timeout=self.config.module_timeout)
                    all_results[module_name] = result
                except Exception as e:
                    self.logger.error(f"å¹¶è¡Œæ‰§è¡Œæ¨¡å— {module_name} å¤±è´¥: {e}")
                    all_results[module_name] = {
                        "status": TestStatus.ERROR,
                        "error": str(e)
                    }
        
        return all_results
    
    def _create_execution_groups(self, modules: List[str]) -> List[List[str]]:
        """åˆ›å»ºæ‰§è¡Œç»„ï¼ˆå¤„ç†ä¾èµ–å…³ç³»ï¼‰"""
        groups = []
        remaining = modules.copy()
        
        while remaining:
            current_group = []
            
            for module in remaining[:]:
                # æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²æ»¡è¶³
                dependencies_satisfied = True
                for dep in self.test_modules[module].dependencies:
                    if dep in remaining:
                        dependencies_satisfied = False
                        break
                
                if dependencies_satisfied:
                    current_group.append(module)
                    remaining.remove(module)
            
            if not current_group:
                # å¾ªç¯ä¾èµ–æˆ–å…¶ä»–é—®é¢˜ï¼Œå¼ºåˆ¶æ·»åŠ å‰©ä½™çš„ç¬¬ä¸€ä¸ª
                current_group.append(remaining.pop(0))
            
            groups.append(current_group)
        
        return groups
    
    async def _generate_comprehensive_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("ğŸ“Š ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        total_duration = time.time() - self.start_time if self.start_time else 0
        
        summary = {
            "execution_time": total_duration,
            "total_modules": len(results),
            "passed_modules": sum(1 for r in results.values() if r.get("status") == TestStatus.PASSED),
            "failed_modules": sum(1 for r in results.values() if r.get("status") == TestStatus.FAILED),
            "error_modules": sum(1 for r in results.values() if r.get("status") == TestStatus.ERROR),
            "total_tests": sum(r.get("total_tests", 0) for r in results.values()),
            "passed_tests": sum(r.get("passed_tests", 0) for r in results.values()),
            "failed_tests": sum(r.get("failed_tests", 0) for r in results.values()),
        }
        
        summary["module_pass_rate"] = (summary["passed_modules"] / summary["total_modules"] * 100) if summary["total_modules"] > 0 else 0
        summary["test_pass_rate"] = (summary["passed_tests"] / summary["total_tests"] * 100) if summary["total_tests"] > 0 else 0
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_data = {
            "summary": summary,
            "results": results,
            "configuration": asdict(self.config),
            "environment": {
                "python_version": sys.version,
                "platform": sys.platform,
                "timestamp": datetime.now().isoformat()
            },
            "performance_metrics": self.performance_monitor.get_metrics()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        await self.reporter.generate_comprehensive_report(report_data)
        
        # è¾“å‡ºæ‘˜è¦åˆ°æ§åˆ¶å°
        self._print_final_summary(summary, results)
    
    def _print_final_summary(self, summary: Dict[str, Any], results: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ¯ Telegramå•†æˆ·æœºå™¨äººV2.0ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        print(f"æ‰§è¡Œæ—¶é—´: {summary['execution_time']:.2f}ç§’")
        print(f"æµ‹è¯•æ¨¡å—: {summary['total_modules']} ä¸ª")
        print(f"é€šè¿‡æ¨¡å—: {summary['passed_modules']} ä¸ª")
        print(f"å¤±è´¥æ¨¡å—: {summary['failed_modules']} ä¸ª")
        print(f"å¼‚å¸¸æ¨¡å—: {summary['error_modules']} ä¸ª")
        print(f"æ¨¡å—é€šè¿‡ç‡: {summary['module_pass_rate']:.1f}%")
        print(f"")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {summary['total_tests']} ä¸ª")
        print(f"é€šè¿‡æµ‹è¯•: {summary['passed_tests']} ä¸ª")
        print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']} ä¸ª")
        print(f"æµ‹è¯•é€šè¿‡ç‡: {summary['test_pass_rate']:.1f}%")
        print("="*80)
        
        # è¯¦ç»†æ¨¡å—ç»“æœ
        print("\nğŸ“‹ å„æ¨¡å—è¯¦ç»†ç»“æœ:")
        for module_name, result in results.items():
            status_emoji = {
                TestStatus.PASSED: "âœ…",
                TestStatus.FAILED: "âŒ",
                TestStatus.ERROR: "ğŸ’¥",
                TestStatus.TIMEOUT: "â°"
            }.get(result.get("status"), "â“")
            
            module_info = self.test_modules.get(module_name, {})
            print(f"{status_emoji} {module_info.get('name', module_name)}")
            
            if "total_tests" in result:
                print(f"   æµ‹è¯•: {result.get('passed_tests', 0)}/{result.get('total_tests', 0)} é€šè¿‡ "
                      f"({result.get('pass_rate', 0):.1f}%)")
            
            if "duration" in result:
                print(f"   è€—æ—¶: {result['duration']:.2f}ç§’")
            
            if result.get("status") in [TestStatus.FAILED, TestStatus.ERROR]:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                print(f"   é”™è¯¯: {error_msg}")
        
        print("="*80)
        
        # æ€»ä½“è¯„ä¼°
        if summary["module_pass_rate"] >= 80:
            print("ğŸ‰ ç»¼åˆæµ‹è¯•è¯„ä¼°: ä¼˜ç§€ (â‰¥80%)")
        elif summary["module_pass_rate"] >= 60:
            print("âš ï¸  ç»¼åˆæµ‹è¯•è¯„ä¼°: è‰¯å¥½ (â‰¥60%)")
        else:
            print("ğŸš¨ ç»¼åˆæµ‹è¯•è¯„ä¼°: éœ€è¦æ”¹è¿› (<60%)")
    
    def _generate_interruption_report(self):
        """ç”Ÿæˆä¸­æ–­æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆä¸­æ–­æŠ¥å‘Š...")
        
        report = {
            "interruption_time": datetime.now().isoformat(),
            "current_module": self.current_module,
            "completed_modules": list(self.result_collector.module_results.keys()),
            "execution_duration": time.time() - self.start_time if self.start_time else 0
        }
        
        # ä¿å­˜ä¸­æ–­æŠ¥å‘Š
        report_file = PROJECT_ROOT / "tests" / "reports" / f"interruption_report_{int(time.time())}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ä¸­æ–­æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # åœæ­¢æ€§èƒ½ç›‘æ§
            self.performance_monitor.stop()
            
            # æ¢å¤æ•°æ®åº“
            await self.db_manager.restore_database()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            await self.db_manager.cleanup_temp_files()
            
            self.logger.info("âœ… æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Telegramå•†æˆ·æœºå™¨äººV2.0ç»¼åˆæµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument("--config", "-c", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--modules", "-m", nargs="+", help="æŒ‡å®šè¦è¿è¡Œçš„æ¨¡å—")
    parser.add_argument("--parallel", "-p", action="store_true", help="å¹¶è¡Œæ‰§è¡Œæ¨¡å—")
    parser.add_argument("--skip-env-check", action="store_true", help="è·³è¿‡ç¯å¢ƒæ£€æŸ¥")
    parser.add_argument("--dry-run", action="store_true", help="ä»…æ£€æŸ¥é…ç½®ä¸æ‰§è¡Œæµ‹è¯•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = ComprehensiveTestRunner(args.config)
    
    try:
        # 1. ç¯å¢ƒæ£€æŸ¥
        if not args.skip_env_check:
            if not await runner.run_environment_check():
                print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·è§£å†³é—®é¢˜åé‡è¯•")
                return 1
        
        # 2. Dry runæ¨¡å¼
        if args.dry_run:
            print("ğŸ” Dry Runæ¨¡å¼ - ä»…æ£€æŸ¥é…ç½®")
            print(f"é…ç½®: {runner.config}")
            print(f"å¯ç”¨æ¨¡å—: {list(runner.test_modules.keys())}")
            return 0
        
        # 3. åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
        if not await runner.initialize_test_environment():
            print("âŒ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")
            return 1
        
        # 4. è¿è¡Œæµ‹è¯•
        results = await runner.run_all_modules(args.modules, args.parallel)
        
        # 5. æ£€æŸ¥æ€»ä½“ç»“æœ
        total_modules = len(results)
        passed_modules = sum(1 for r in results.values() if r.get("status") == TestStatus.PASSED)
        
        if passed_modules == total_modules:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•æ¨¡å—é€šè¿‡ï¼")
            return 0
        else:
            print(f"âš ï¸ {total_modules - passed_modules}/{total_modules} ä¸ªæ¨¡å—æœªé€šè¿‡")
            return 1
    
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    
    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        return 1
    
    finally:
        # æ¸…ç†èµ„æº
        await runner.cleanup()

if __name__ == "__main__":
    # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ï¼ˆWindowså…¼å®¹æ€§ï¼‰
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    # è¿è¡Œä¸»å‡½æ•°
    exit_code = asyncio.run(main())
    sys.exit(exit_code)